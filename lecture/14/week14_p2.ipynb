{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 14, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Deep Learning and Wrap Up\n",
    "#### Many pretty pictures are borrowed from Federica Bianco\n",
    "\n",
    "### Gautham Narayan \n",
    "\n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Method</th>\n",
    "    <th>Unsupervised</th>\n",
    "    <th>Supervised</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Centroid-based</td>\n",
    "    <td>k-Means</td>\n",
    "    <td>kNN w/ k=1 (aka Nearest Centroid) </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Linkage-based Model</td>\n",
    "    <td>Hierarchical Clustering</td>\n",
    "    <td>Decision Trees/Random Forests</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Parametric Density Model</td>\n",
    "    <td>GMMs, Extreme Deconvolution</td>\n",
    "    <td>Gaussian Naive Bayes, LDA, QDA</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Nonparametric Density/Locally Weighted*</td>\n",
    "    <td>DBSCAN/Optics</td>\n",
    "      <td>kNN with k>1, <strong>SVM, MLPs, DNN</strong></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "* = KDEs are perfectly useable in either column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And now for biology!!\n",
    "\n",
    "\n",
    "<img src=\"bioneuron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Signals are responses to stimuli caused by changing the balanace of Na+ and K+ ions within the Axon (which in turn changes the bias voltage between the terminal and dendrite) - allows a signal to be transmitted down the axon.\n",
    "\n",
    "<img src=\"neuronal_action_potential.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Action_Potential_Propagation_Along_Axon_Animation.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"myelinated-axon.gif\">\n",
    "\n",
    "If this is interesting, you can look at a more detailed explanation here: https://www.getbodysmart.com/nervous-system/action-potential-events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"McCulloughPitts1943.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"bioneuron2.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"mp_neuron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## OK, but how do we get the weights when this is linear regression, but the output is binary (0, 1)\n",
    "\n",
    "### Recall the discriminant function from Tuesday:\n",
    "\n",
    "\n",
    "## $$\\begin{eqnarray} \\widehat{y} & = & \\left\\{ \\begin{array}{cl} 1 & \\mbox{if $g(x) > \\theta$}, \\\\ 0 & \\mbox{otherwise,} \\end{array} \\right. \\end{eqnarray}$$\n",
    "\n",
    "Where $\\theta$ is the **threshold**\n",
    "\n",
    "We compare the output vector $y$ to the target/truth $t$ and we want to minimize the error\n",
    "\n",
    "## $$ E = 1/2 \\cdot (t-y)^2$$\n",
    "\n",
    "To minimize the error, we need to change the weights of the inputs to get $t$ close to $y$.\n",
    "\n",
    "Let's do this with our old friend, **gradient descent**\n",
    "\n",
    "## $$w_i = w'_i - \\eta \\cdot \\frac{dE}{dw_i} $$\n",
    "\n",
    "Where $\\eta$ is some constant that we'll call the **learning rate**\n",
    "\n",
    "## $$w_i = w'_i + \\eta \\cdot (t-y) \\cdot\\frac{dy}{dw_i} $$\n",
    "\n",
    "## $$\\frac{dy}{dw_i} = x_i $$\n",
    "\n",
    "so..\n",
    "\n",
    "## $$w_i = w'_i + \\eta \\cdot (t-y) \\cdot x_i $$\n",
    "\n",
    "\n",
    "If $y != t$ (and remember the t is just a 0 or 1):\n",
    "## $$ w_{\\text{new}}  = w_{\\text{old}} + \\eta \\cdot t \\cdot x$$\n",
    "\n",
    "and $y == t$, don't update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"mk1_perceptron.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"1960_preceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Mark_I_perceptron.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"perceptron6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \n",
    "\n",
    "- The Rosenblatt Perceptron uses the class labels to learn model coefficients\n",
    "\n",
    "- The Adaline Neuron uses continuous predicted values (from the net input) to learn the model coefficients, which is more “powerful” since it tells us by “how much” we were right or wrong\n",
    "    - the weights are how sensitive a neuron is \n",
    "    - the activation function turns neurons on/off \n",
    "    - The loss function of the network is updated to penalize models in proportion to the magnitude of their activation\n",
    "    - prevents updating weights when y gets close to t - i.e. prevents overfitting!\n",
    "\n",
    "    \n",
    "## $$w_i = w'_i + \\eta \\cdot (t-y) \\cdot\\frac{df(y)}{dw_i} $$\n",
    "\n",
    "<img src=\"adaline_vs_perceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"activation_functions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-not actually class-class exercise:\n",
    "\n",
    "https://jalammar.github.io/feedforward-neural-networks-visual-interactive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"mlp1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mlp2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise: Implementing a simple MLP Classifier with `sklearn`\n",
    "\n",
    "Our neural network will take an $\\mathbf{x} = (x_1, x_2)$ vector as input and output a $K$-dimensional vector $\\mathbf{p}=(p_1,\\dots,p_K)$ of class probabilities. For simplicity we'll focus on a single choice of activation function, the ReLU function $f(x) = \\max(x, 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports / plotting configuration\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # hard classification boundaries\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spiral sample data\n",
    "# N = num points\n",
    "# K = num spiral arms\n",
    "# sigma = some noise\n",
    "def spiral_data(N, K=3, sigma=0.1):\n",
    "    X = np.zeros((N * K, 2))\n",
    "    y = np.zeros(N * K, dtype='int')\n",
    "\n",
    "    for j in range(K):\n",
    "        ix = range(N * j, N * (j + 1))\n",
    "        r = np.linspace(0.0, 1, N)  # radius\n",
    "        theta = 2 * np.pi * j / K + np.linspace(0, 3 * np.pi, N) + np.random.randn(N) * sigma\n",
    "        X[ix] = np.c_[r * np.sin(theta), r * np.cos(theta)]\n",
    "        y[ix] = j\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "K = 3\n",
    "X, y = spiral_data(N, K, 0.1)\n",
    "\n",
    "# Visualize the generated data\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='viridis')\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-1, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SingleLayerReLU(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Skeleton code for single-layer multi-class neural network classifier w/ ReLU activation.\n",
    "    NOTE: Whenever you change the code below, you need to re-run this cell AND re-initialize\n",
    "    your model (`model = SingleLayerNet(...)`) in order to update your specific `model` object.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_classes, sigma_init=0.01):\n",
    "        \"\"\"Initialize weights with Gaussian noise scaled by `sigma_init` and\n",
    "        biases with zeros.\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.W1 = sigma_init * np.random.randn(hidden_size, 2)\n",
    "        self.W2 = sigma_init * np.random.randn(num_classes, hidden_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.b2 = np.zeros(num_classes)\n",
    "    \n",
    "    def loss(self, y, P):\n",
    "        \"\"\"Compute total softmax loss.\n",
    "        Inputs:  y -> (N,) array of true (integer) labels\n",
    "                 p -> (N, K) array of predicted probabilities\n",
    "        Outputs: L -> total loss value       \n",
    "        \"\"\"\n",
    "        return -np.sum(np.log(P[range(len(P)), y]))\n",
    "        \n",
    "    def dloss(self, X, y):\n",
    "        \"\"\"Compute gradient of softmax loss with respect to network weights.\n",
    "        Inputs:  X -> (N, 2) array of network inputs\n",
    "                 y -> (N,) array of true labels\n",
    "        Outputs: dW1 -> (hidden_size, 2) array of weight derivatives\n",
    "                 dW2 -> (hidden_size, 2) array of weight derivatives\n",
    "                 db1 -> (hidden_size,) array of bias derivatives\n",
    "                 db2 -> (hidden_size,) array of bias derivatives\n",
    "        \"\"\"\n",
    "        H = np.maximum(0, X @ self.W1.T + self.b1)  # ReLU activation\n",
    "        Z = H @ self.W2.T + self.b2\n",
    "        P = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "\n",
    "        dZ = P\n",
    "        dZ[range(len(X)), y] -= 1\n",
    "\n",
    "        dW2 = (H.T @ dZ).T\n",
    "        db2 = np.sum(dZ, axis=0)\n",
    "\n",
    "        dH = dZ @ self.W2\n",
    "        dH[H <= 0] = 0  # backprop ReLU  activation\n",
    "\n",
    "        dW1 = (X.T @ dH).T\n",
    "        db1 = np.sum(dH, axis=0)\n",
    "        \n",
    "        return (dW1, dW2, db1, db2)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Compute forward pass for all input values.\n",
    "        Inputs:  X -> (N, 2) array of network inputs\n",
    "        Outputs: P -> (N, K) array of class probabilities\n",
    "        \"\"\"\n",
    "        H = np.maximum(0, X @ self.W1.T + self.b1)  # ReLU activation\n",
    "        Z = H @ self.W2.T + self.b2\n",
    "        P = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        \n",
    "        return P\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Compute most likely class labels for all input values.\n",
    "        Inputs:  X -> (N, 2) array of network inputs\n",
    "        Outputs: P -> (N, K) array of class probabilities\n",
    "        \"\"\"\n",
    "        P = self.predict_proba(X)\n",
    "        return np.argmax(P, 1)\n",
    "        \n",
    "    def fit(self, X, y, step_size=3e-3, n_iter=10000):\n",
    "        \"\"\"Optimize model parameters W1, W2, b1, b2 via gradient descent.\n",
    "        Inputs:  X -> (N, 2) array of network inputs\n",
    "                 y -> (N,) array of true labels\n",
    "                 step_size -> gradient descent step size\n",
    "                 n_iter -> number of gradient descent steps to perform\n",
    "        Outputs: losses -> (n_iter,) array of loss values after each step\n",
    "        \"\"\"\n",
    "        losses = np.zeros(n_iter + 1)\n",
    "        for i in range(0, n_iter + 1):\n",
    "            dW1, dW2, db1, db2 = self.dloss(X, y)\n",
    "            self.W1 -= step_size * dW1\n",
    "            self.W2 -= step_size * dW2\n",
    "            self.b1 -= step_size * db1\n",
    "            self.b2 -= step_size * db2\n",
    "            \n",
    "            P = self.predict_proba(X)\n",
    "            losses[i] = self.loss(y, P)\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(\"Iteration {}: loss={}\".format(i, losses[i]))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, X, y, step=0.02):\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"Accuracy: {}%\".format(100 * np.mean(y == model.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "model = SingleLayerReLU(100, K)\n",
    "visualize_predictions(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleLayerReLU(100, K)\n",
    "losses = model.fit(X, y, step_size=2e-3, n_iter=20000)\n",
    "plt.plot(losses, '-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Total loss');\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X, y = spiral_data(100, 5, 0.1)\n",
    "single_layer_model = MLPClassifier((8,), activation='identity', solver='lbfgs')\n",
    "single_layer_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(single_layer_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_layer_model = MLPClassifier((100, 100), alpha=0.5, activation='relu', solver='lbfgs')\n",
    "multi_layer_model.fit(X, y)\n",
    "visualize_predictions(multi_layer_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for later\n",
    "## [http://playground.tensorflow.org](http://playground.tensorflow.org/#dataset=spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise: Implementing a simple MLP Regressor with `sklearn`\n",
    "\n",
    "### Predict house prices in Boston with an MLP regressor\n",
    "### Use `cross_val_predict` to see how well you do after 5-fold cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute this cell a\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why have one hidden layer when you can have two for twice the price!\n",
    "\n",
    "<img src=\"dnn1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dnn2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of neural networks\n",
    "## - \"Neural Network Zoo\" (Asimov Institute)\n",
    "<img src=\"http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly much more sophisticated than our basic perceptron. \"Deep\" networks consist of tens of layers with thousands of neurons. These large networks have become usabel thanks to two breakthroughs: the use of sparse layers and the power of graphics processing units (GPUs).\n",
    "Many image processing tasks involve convolving an image with a 2-dimensional kernel as shown below.\n",
    "![Convolution example](https://developer.apple.com/library/content/documentation/Performance/Conceptual/vImage/Art/kernel_convolution.jpg)\n",
    "\n",
    "The sparse layers or convolutional layers in a deep network contain a large number of hidden nodes but very few synapses. The sparseness arises from the relatively small size of a typical convolution kernel (15x15 is a large kernel), so a hidden node representing one output of the convolution is connected to only a few input nodes. Compare this the our previous perceptron, in which every hidden node was connected to every input node.\n",
    "\n",
    "Even though the total number of connections is greatly reduced in the sparse layers, the total number of nodes and connections in a modern deep network is still enormous. Luckily, training these networks turns out to be a great task for GPU acceleration! Serious work using neural networks is almost always done usign specialized GPU-accelerated platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning frameworks\n",
    "\n",
    "## - Python(-compatible) - like sklearn only for NN models:\n",
    "- Tensorflow (Google)\n",
    "- Keras (frontend for TensorFlow + Theano)\n",
    "- Theano (Université de Montréal)\n",
    "- Caffe (UC Berkeley)\n",
    "- CNTK (Microsoft)\n",
    "- MXNet (Amazon+Baidu+...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mindmap.jpg\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
