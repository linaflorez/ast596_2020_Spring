{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 11, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Intro to Machine Learning\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> If you are still working on HW7 and having issues with the chains, thing about priors for the mean and offsets + period and the lengthscale of the quasi-periodic variations. </center>\n",
    "    \n",
    "### In particular remember that if period=0, log(period) = -inf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "(Skipping Approximate Bayesian Computation on your syllabus - we may come back later if we have time)\n",
    "\n",
    "- We've moved from working on single-level models to multi-level/hierarchical models \n",
    "- We've covered techniques were we explicitly write down a paramettric mdoel to \"non-parametric\" methods (which is weird because these methods often have more parameters than parametric methods, but they're all nuisance parameters and constrained only because there is a hierarchical structure with hyperpriors)\n",
    "\n",
    "Both these trends are essentially driven by the growth in dataset size *and complexity* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Machine Learning ?\n",
    "\n",
    "* The umbrella term \"machine learning\" describes methods for *automated data analysis*, developed by computer scientists and statisticians in response to the appearance of ever larger datasets.\n",
    "\n",
    "* What is actually being learned? With GPs, you **specified** the functional form for the correlation between observations in your training set. With ML, you specify a notion of how to measure the distance between observations, and it learns the correlation structure and builds a model, $M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The goal of automation has led to an emphasis on non-parametric models (that adapt to dataset size and complexity), and a very uniform terminology that enables multiple models to be implemented and compared on an equal footing.\n",
    "\n",
    "* Machine learning can be divided into two types: *supervised* and *unsupervised.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Supervised learning is also known as *predictive* learning. Given *inputs* $X$, the goal is to construct a machine that can accurately predict a set of *outputs* $y$, usually so that _decisions_ can be made. \n",
    "\n",
    "\n",
    "* The \"supervision\" refers to the education of the machine, via a *training set* $D$ of input-output pairs that we provide. Prediction accuracy is then tested on *validation* and *test* sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* At the heart of the prediction machine is a *model* $M$ that can be *trained* to give accurate predictions.\n",
    "\n",
    "* Supervised learning is about making predictions by characterizing ${\\rm Pr}(y_k|x_k,D,M)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* The outputs $y$ are said to be *response variables* - predictions of $y$ will be generated by our model. \n",
    "\n",
    "* The variables $y$ can be either *categorical* (\"labels\") or *nominal* (real numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* When the $y$ are categorical, the problem is one of *classification* (\"is this an image of a `dog`, or my `dinner`?\"). \n",
    "\n",
    "<img src=\"dog_or_dinner.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* When the $y$ are numerical, the problem is a *regression* (\"how should we interpolate between these numerical values?\").\n",
    "\n",
    "<img src=\"house_price_features.png\">\n",
    "\n",
    "<img src=\"house_price_features_corr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "* Also known as *descriptive* learning. Here the goal is \"knowledge discovery\" - detection of patterns in a dataset, that can then be used in supervised/model-based analyses. \n",
    "\n",
    "\n",
    "* Unsupervised learning is about *density estimation* - characterizing ${\\rm Pr}(x|\\theta,H)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "* Examples of unsupervised learning activities include:\n",
    "\n",
    "  * Clustering analysis of the $x$.\n",
    "  * Dimensionality reduction: principal component analysis, independent component analysis, etc.\n",
    "  \n",
    "  \n",
    "<img src=\"ul_stocks_all.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_clusters.png\">\n",
    "\n",
    "Credit: Lorien Hayden (Cornell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_sectors.png\">\n",
    "\n",
    "Credit: Lorien Hayden (Cornell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_stocks.png\">\n",
    "\n",
    "Credit: Lorien Hayden (Cornell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_portfolio.png\">\n",
    "\n",
    "Credit: Lorien Hayden (Cornell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## But of course individual members of a population can look wildly different, so clustering is nice, but we want our  regression and forecasting infrastructure as well. \n",
    "\n",
    "## Even these can be affected by unforseen circumstances though.\n",
    "\n",
    "<img src=\"ul_stocks_after_data_breach.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ul_vs_sl1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_vs_sl2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_vs_sl3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_vs_sl4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_vs_sl5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_vs_sl6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ul_vs_sl7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"ml_map.png\"></img>\n",
    "\n",
    "> The [`scikit-learn` algorithm cheatsheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), as provided with the package documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Representations\n",
    "\n",
    "* Each input $x$ is said to have $P$ *features* (or *attributes*), and represents a *sample* (assumed to have been drawn from a sampling distribution). Each sample input $x$ is associated with an output $y$.\n",
    "\n",
    "\n",
    "* Our $N$ input *samples* are packaged into an $N \\times P$ *design matrix* $X$ (with $N$ rows and $P$ columns). We've used this term before in the context of regression and you saw an example of building one with the HBM on Cepheids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ml_data_representation.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Typically a supervised learning model is \"trained\" on a subset of the data, and then its ability to make predictions about new data \"tested\" on the remainder.\n",
    "\n",
    "* Training involves \"fitting\" the model to the data, optimizing its parameters to minimize some \"loss function\" (or equivalently, maximize some defined \"score\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ml_supervised_workflow.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"ml_train_test_split_matrix.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Models\n",
    "\n",
    "Examples of data-driven, non-parametric models for use in supervised learning include K-nearest neighbors, Support Vector Machines, Random Forest, Neural Networks, and many more. \n",
    "\n",
    "Many can be used for either classification or regression.\n",
    "\n",
    "All have a number of **hyper-parameters** that govern their overall behavior, that need to be determined for any given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizing Model Prediction Accuracy\n",
    "\n",
    "* In supervised machine learning the goal is to make the most accurate predictions we can - which means neither over-fitting nor under-fitting the data \n",
    "\n",
    "* The \"mean squared error\" between the model predictions and the truth is a useful metric: minimizing MSE corresponds to minimizing the \"empirical risk,\" defined as the mean value loss function averaged over the available data samples, where the loss function is quadratic\n",
    "\n",
    "<img src=\"overfitting_underfitting_cartoon.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\\;\\;\\;\\;\\;{\\rm MSE} = \\mathcal{E} \\left[ (\\hat{y} - y^{\\rm true})^2 \\right] = \\mathcal{E} \\left[ (\\hat{y} - \\bar{y} + \\bar{y} - y^{\\rm true})^2 \\right] = \\mathcal{E} \\left[ (\\hat{y} - \\bar{y})^2 \\right] + (\\bar{y} - y^{\\rm true})^2$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = {\\rm var}(\\hat{y}) + {\\rm bias}^2(\\hat{y})$\n",
    "\n",
    "\n",
    "* In general, different models reach different balances between the variance and bias of their predictions\n",
    "\n",
    "* A particular choice of loss function leads to a corresponding minimized risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "* With a single training/test split, one can characterize the _prediction error_ using, for example, the MSE. \n",
    "\n",
    "* The model that minimizes the *generalized prediction error* can be found (approximately) with *cross validation*, in which we consider multiple training/test splits, and look at the _mean prediction error_ across all of these _\"folds.\"_\n",
    "\n",
    "* How we design the folds matters: we want each subset of the data to be a _fair sample_ of the whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml_grid_search_cross_validation.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another layer of cross validation is still needed, since we need to guard against over-fitting to this particular training set: we need to try all possible training sets.\n",
    "\n",
    "* Once we have the hyperparameters that optimize the generalized prediction error, we can then fix them at their optimal values and train on model on the entire data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat Emptor\n",
    "\n",
    "* Machine learning algorithms are designed to make good use of big, complex datasets, where there are likely to be many more correlations and connections than we have thought of yet. \n",
    "\n",
    "\n",
    "* In this approach we assume that we will be able to make better predictions by using flexible, \"non-parametric\" methods that scale with the size of the dataset and allow new relationships to emerge empirically\n",
    "\n",
    "\n",
    "* Additional work needs to be done to extract a full Bayesian posterior PDF (or even frequentist confidence intervals) for the model parameters - which are typically not the focus of a machine learning analysis."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
